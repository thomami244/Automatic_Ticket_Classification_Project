{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"provenance":[],"collapsed_sections":["yYzD85nTJrGA","piyLxzj6v07j","280Vbqk-7a8M"],"gpuType":"A100"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"rhR-ZUkwJrFn"},"source":["## Problem Statement\n","\n","You need to build a model that is able to classify customer complaints based on the products/services. By doing so, you can segregate these tickets into their relevant categories and, therefore, help in the quick resolution of the issue.\n","\n","You will be doing topic modelling on the <b>.json</b> data provided by the company. Since this data is not labelled, you need to apply NMF to analyse patterns and classify tickets into the following five clusters based on their products/services:\n","\n","* Credit card / Prepaid card\n","\n","* Bank account services\n","\n","* Theft/Dispute reporting\n","\n","* Mortgages/loans\n","\n","* Others\n","\n","\n","With the help of topic modelling, you will be able to map each ticket onto its respective department/category. You can then use this data to train any supervised model such as logistic regression, decision tree or random forest. Using this trained model, you can classify any new customer complaint support ticket into its relevant department."]},{"cell_type":"markdown","metadata":{"id":"mcgXVNyaLUFS"},"source":["## Pipelines that needs to be performed:\n","\n","You need to perform the following eight major tasks to complete the assignment:\n","\n","1.  Data loading\n","\n","2. Text preprocessing\n","\n","3. Exploratory data analysis (EDA)\n","\n","4. Feature extraction\n","\n","5. Topic modelling\n","\n","6. Model building using supervised learning\n","\n","7. Model training and evaluation\n","\n","8. Model inference"]},{"cell_type":"markdown","metadata":{"id":"JuLFIymAL58u"},"source":["## Importing the necessary libraries"]},{"cell_type":"code","metadata":{"id":"O-Q9pqrcJrFr"},"source":["import json\n","import numpy as np\n","import pandas as pd\n","import re, nltk, spacy, string\n","import en_core_web_sm\n","nlp = en_core_web_sm.load()\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","from plotly.offline import plot\n","import plotly.graph_objects as go\n","import plotly.express as px\n","\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from pprint import pprint\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","import re\n","import string\n","import spacy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KtRLCsNVJrFt"},"source":["## Loading the data\n","\n","The data is in JSON format and we need to convert it to a dataframe."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kmlWqdnxwual","executionInfo":{"status":"ok","timestamp":1740352229106,"user_tz":-480,"elapsed":2701,"user":{"displayName":"Michael Thomas","userId":"16912601426565409661"}},"outputId":"1a0c3d84-8c7e-41c8-a22d-0198bf8c93a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!ls  \"/content/drive/My Drive/Automatic_Ticket_Classification_Project/Project_data/\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rq_GdAxfxWyE","executionInfo":{"status":"ok","timestamp":1740352229172,"user_tz":-480,"elapsed":65,"user":{"displayName":"Michael Thomas","userId":"16912601426565409661"}},"outputId":"468f6e0d-9201-494c-c0d7-28c3c562a8e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["complaints-2021-05-14_08_16.json\n"]}]},{"cell_type":"code","metadata":{"id":"puVzIf_iJrFt"},"source":["# Opening JSON file\n","# f =  \"/content/drive/My Drive/Automatic_Ticket_Classification_Project/Project_data/complaints-2021-05-14_08_16.json\"# Write the path to your data file and load it\n","\n","# # returns JSON object as\n","# # a dictionary\n","# data = json.load(f)\n","# df=pd.json_normalize(data)\n","\n","file_path = \"/content/drive/My Drive/Automatic_Ticket_Classification_Project/Project_data/complaints-2021-05-14_08_16.json\"\n","with open(file_path, \"r\") as f:\n","    data = json.load(f)\n","df=pd.json_normalize(data)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_xYpH-sAJrFu"},"source":["## Data preparation"]},{"cell_type":"code","metadata":{"id":"Lf8ufHH5JrFu","colab":{"base_uri":"https://localhost:8080/","height":394},"executionInfo":{"status":"ok","timestamp":1740352231195,"user_tz":-480,"elapsed":20,"user":{"displayName":"Michael Thomas","userId":"16912601426565409661"}},"outputId":"eedeb0c2-e4b1-4d4e-f523-c0ce9b7d4921"},"source":["# Inspect the dataframe to understand the given data.\n","\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                _index      _type      _id  _score   _source.tags  \\\n","0  complaint-public-v2  complaint  3211475     0.0           None   \n","1  complaint-public-v2  complaint  3229299     0.0  Servicemember   \n","2  complaint-public-v2  complaint  3199379     0.0           None   \n","3  complaint-public-v2  complaint  2673060     0.0           None   \n","4  complaint-public-v2  complaint  3203545     0.0           None   \n","\n","  _source.zip_code _source.complaint_id                       _source.issue  \\\n","0            90301              3211475   Attempts to collect debt not owed   \n","1            319XX              3229299     Written notification about debt   \n","2            77069              3199379  Other features, terms, or problems   \n","3            48066              2673060      Trouble during payment process   \n","4            10473              3203545                    Fees or interest   \n","\n","       _source.date_received _source.state  ... _source.company_response  \\\n","0  2019-04-13T12:00:00-05:00            CA  ...  Closed with explanation   \n","1  2019-05-01T12:00:00-05:00            GA  ...  Closed with explanation   \n","2  2019-04-02T12:00:00-05:00            TX  ...  Closed with explanation   \n","3  2017-09-13T12:00:00-05:00            MI  ...  Closed with explanation   \n","4  2019-04-05T12:00:00-05:00            NY  ...  Closed with explanation   \n","\n","        _source.company _source.submitted_via _source.date_sent_to_company  \\\n","0  JPMORGAN CHASE & CO.                   Web    2019-04-13T12:00:00-05:00   \n","1  JPMORGAN CHASE & CO.                   Web    2019-05-01T12:00:00-05:00   \n","2  JPMORGAN CHASE & CO.                   Web    2019-04-02T12:00:00-05:00   \n","3  JPMORGAN CHASE & CO.                   Web    2017-09-14T12:00:00-05:00   \n","4  JPMORGAN CHASE & CO.              Referral    2019-04-05T12:00:00-05:00   \n","\n","  _source.company_public_response                         _source.sub_product  \\\n","0                            None                            Credit card debt   \n","1                            None                            Credit card debt   \n","2                            None  General-purpose credit card or charge card   \n","3                            None                  Conventional home mortgage   \n","4                            None  General-purpose credit card or charge card   \n","\n","  _source.timely                    _source.complaint_what_happened  \\\n","0            Yes                                                      \n","1            Yes  Good morning my name is XXXX XXXX and I apprec...   \n","2            Yes  I upgraded my XXXX XXXX card in XX/XX/2018 and...   \n","3            Yes                                                      \n","4            Yes                                                      \n","\n","                                  _source.sub_issue  \\\n","0                                 Debt is not yours   \n","1  Didn't receive enough information to verify debt   \n","2             Problem with rewards from credit card   \n","3                                              None   \n","4                         Charged too much interest   \n","\n","  _source.consumer_consent_provided  \n","0              Consent not provided  \n","1                  Consent provided  \n","2                  Consent provided  \n","3              Consent not provided  \n","4                               N/A  \n","\n","[5 rows x 22 columns]"],"text/html":["\n","  <div id=\"df-df9724f8-c893-42f8-8165-6ea14ad5dc74\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>_index</th>\n","      <th>_type</th>\n","      <th>_id</th>\n","      <th>_score</th>\n","      <th>_source.tags</th>\n","      <th>_source.zip_code</th>\n","      <th>_source.complaint_id</th>\n","      <th>_source.issue</th>\n","      <th>_source.date_received</th>\n","      <th>_source.state</th>\n","      <th>...</th>\n","      <th>_source.company_response</th>\n","      <th>_source.company</th>\n","      <th>_source.submitted_via</th>\n","      <th>_source.date_sent_to_company</th>\n","      <th>_source.company_public_response</th>\n","      <th>_source.sub_product</th>\n","      <th>_source.timely</th>\n","      <th>_source.complaint_what_happened</th>\n","      <th>_source.sub_issue</th>\n","      <th>_source.consumer_consent_provided</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>complaint-public-v2</td>\n","      <td>complaint</td>\n","      <td>3211475</td>\n","      <td>0.0</td>\n","      <td>None</td>\n","      <td>90301</td>\n","      <td>3211475</td>\n","      <td>Attempts to collect debt not owed</td>\n","      <td>2019-04-13T12:00:00-05:00</td>\n","      <td>CA</td>\n","      <td>...</td>\n","      <td>Closed with explanation</td>\n","      <td>JPMORGAN CHASE &amp; CO.</td>\n","      <td>Web</td>\n","      <td>2019-04-13T12:00:00-05:00</td>\n","      <td>None</td>\n","      <td>Credit card debt</td>\n","      <td>Yes</td>\n","      <td></td>\n","      <td>Debt is not yours</td>\n","      <td>Consent not provided</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>complaint-public-v2</td>\n","      <td>complaint</td>\n","      <td>3229299</td>\n","      <td>0.0</td>\n","      <td>Servicemember</td>\n","      <td>319XX</td>\n","      <td>3229299</td>\n","      <td>Written notification about debt</td>\n","      <td>2019-05-01T12:00:00-05:00</td>\n","      <td>GA</td>\n","      <td>...</td>\n","      <td>Closed with explanation</td>\n","      <td>JPMORGAN CHASE &amp; CO.</td>\n","      <td>Web</td>\n","      <td>2019-05-01T12:00:00-05:00</td>\n","      <td>None</td>\n","      <td>Credit card debt</td>\n","      <td>Yes</td>\n","      <td>Good morning my name is XXXX XXXX and I apprec...</td>\n","      <td>Didn't receive enough information to verify debt</td>\n","      <td>Consent provided</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>complaint-public-v2</td>\n","      <td>complaint</td>\n","      <td>3199379</td>\n","      <td>0.0</td>\n","      <td>None</td>\n","      <td>77069</td>\n","      <td>3199379</td>\n","      <td>Other features, terms, or problems</td>\n","      <td>2019-04-02T12:00:00-05:00</td>\n","      <td>TX</td>\n","      <td>...</td>\n","      <td>Closed with explanation</td>\n","      <td>JPMORGAN CHASE &amp; CO.</td>\n","      <td>Web</td>\n","      <td>2019-04-02T12:00:00-05:00</td>\n","      <td>None</td>\n","      <td>General-purpose credit card or charge card</td>\n","      <td>Yes</td>\n","      <td>I upgraded my XXXX XXXX card in XX/XX/2018 and...</td>\n","      <td>Problem with rewards from credit card</td>\n","      <td>Consent provided</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>complaint-public-v2</td>\n","      <td>complaint</td>\n","      <td>2673060</td>\n","      <td>0.0</td>\n","      <td>None</td>\n","      <td>48066</td>\n","      <td>2673060</td>\n","      <td>Trouble during payment process</td>\n","      <td>2017-09-13T12:00:00-05:00</td>\n","      <td>MI</td>\n","      <td>...</td>\n","      <td>Closed with explanation</td>\n","      <td>JPMORGAN CHASE &amp; CO.</td>\n","      <td>Web</td>\n","      <td>2017-09-14T12:00:00-05:00</td>\n","      <td>None</td>\n","      <td>Conventional home mortgage</td>\n","      <td>Yes</td>\n","      <td></td>\n","      <td>None</td>\n","      <td>Consent not provided</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>complaint-public-v2</td>\n","      <td>complaint</td>\n","      <td>3203545</td>\n","      <td>0.0</td>\n","      <td>None</td>\n","      <td>10473</td>\n","      <td>3203545</td>\n","      <td>Fees or interest</td>\n","      <td>2019-04-05T12:00:00-05:00</td>\n","      <td>NY</td>\n","      <td>...</td>\n","      <td>Closed with explanation</td>\n","      <td>JPMORGAN CHASE &amp; CO.</td>\n","      <td>Referral</td>\n","      <td>2019-04-05T12:00:00-05:00</td>\n","      <td>None</td>\n","      <td>General-purpose credit card or charge card</td>\n","      <td>Yes</td>\n","      <td></td>\n","      <td>Charged too much interest</td>\n","      <td>N/A</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 22 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-df9724f8-c893-42f8-8165-6ea14ad5dc74')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-df9724f8-c893-42f8-8165-6ea14ad5dc74 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-df9724f8-c893-42f8-8165-6ea14ad5dc74');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-e111ddc3-7f17-43cf-af65-041882c8a10a\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e111ddc3-7f17-43cf-af65-041882c8a10a')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-e111ddc3-7f17-43cf-af65-041882c8a10a button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df"}},"metadata":{},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"Dwcty-wmJrFw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740352231199,"user_tz":-480,"elapsed":3,"user":{"displayName":"Michael Thomas","userId":"16912601426565409661"}},"outputId":"2a13ab0d-2bd3-4660-e06d-dc5fb4069e6e"},"source":["#print the column names\n","df.columns"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['_index', '_type', '_id', '_score', '_source.tags', '_source.zip_code',\n","       '_source.complaint_id', '_source.issue', '_source.date_received',\n","       '_source.state', '_source.consumer_disputed', '_source.product',\n","       '_source.company_response', '_source.company', '_source.submitted_via',\n","       '_source.date_sent_to_company', '_source.company_public_response',\n","       '_source.sub_product', '_source.timely',\n","       '_source.complaint_what_happened', '_source.sub_issue',\n","       '_source.consumer_consent_provided'],\n","      dtype='object')"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"FYCtKXD1JrFw"},"source":["#Assign new column names\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(df[[\"_source.complaint_what_happened\", \"_source.product\"]])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zRo7BT-r4Ljv","executionInfo":{"status":"ok","timestamp":1740352231212,"user_tz":-480,"elapsed":12,"user":{"displayName":"Michael Thomas","userId":"16912601426565409661"}},"outputId":"18a9fc97-e383-47e2-8206-f3f987aa1d08"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                         _source.complaint_what_happened  \\\n","0                                                          \n","1      Good morning my name is XXXX XXXX and I apprec...   \n","2      I upgraded my XXXX XXXX card in XX/XX/2018 and...   \n","3                                                          \n","4                                                          \n","...                                                  ...   \n","78308                                                      \n","78309  On Wednesday, XX/XX/XXXX I called Chas, my XXX...   \n","78310  I am not familiar with XXXX pay and did not un...   \n","78311  I have had flawless credit for 30 yrs. I've ha...   \n","78312  Roughly 10+ years ago I closed out my accounts...   \n","\n","                   _source.product  \n","0                  Debt collection  \n","1                  Debt collection  \n","2      Credit card or prepaid card  \n","3                         Mortgage  \n","4      Credit card or prepaid card  \n","...                            ...  \n","78308  Checking or savings account  \n","78309  Credit card or prepaid card  \n","78310  Checking or savings account  \n","78311  Credit card or prepaid card  \n","78312                  Payday loan  \n","\n","[78313 rows x 2 columns]\n"]}]},{"cell_type":"markdown","source":["It appears that the customer complaints are in this column: \"_source.complaint_what_happened\" and the product that it relates to is in the column  \"_source.product\". The other columns do not look relevant to this exercise. Consequently we will only assign the name for these two columns.\n","\n"],"metadata":{"id":"1Q1T824-3euE"}},{"cell_type":"code","source":["df.rename(columns={'_source.complaint_what_happened':'complaint_what_happened', '_source.product':'tag'}, inplace=True)"],"metadata":{"id":"SO_Q4cZR49iD"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"grQUPFL5JrFx"},"source":["#Assign nan in place of blanks in the complaints column\n","df['complaint_what_happened'] = df['complaint_what_happened'].replace(\"\", np.nan)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jfxd8VSmJrFy"},"source":["#Remove all rows where complaints column is nan\n","df.dropna(subset=['complaint_what_happened'], inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking the shape of the dataframe again\n","df.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0s_-Lxc27gOp","executionInfo":{"status":"ok","timestamp":1740352231228,"user_tz":-480,"elapsed":3,"user":{"displayName":"Michael Thomas","userId":"16912601426565409661"}},"outputId":"a6564f4a-5326-4ccc-a60c-1c09b8ba5fac"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(21072, 22)"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["Conclusion: The number of rows have reduced from 78313 to 21072 due to the blank customer complains being removed"],"metadata":{"id":"ly-D-b-V74sC"}},{"cell_type":"markdown","metadata":{"id":"L944HZpsJrFy"},"source":["## Prepare the text for topic modeling\n","\n","Once you have removed all the blank complaints, you need to:\n","\n","* Make the text lowercase\n","* Remove text in square brackets\n","* Remove punctuation\n","* Remove words containing numbers\n","\n","\n","Once you have done these cleaning operations you need to perform the following:\n","* Lemmatize the texts\n","* Extract the POS tags of the lemmatized text and remove all the words which have tags other than NN[tag == \"NN\"].\n"]},{"cell_type":"code","metadata":{"id":"qm7SjjSkJrFz"},"source":["# Write your function here to clean the text and remove all the unnecessary elements.\n","\n","\n","def preprocess_text(text):\n","    \"\"\"\n","    Cleans the input text by:\n","    - Converting it to lowercase\n","    - Removing text within square brackets\n","    - Eliminating punctuation\n","    - Filtering out words that contain numbers\n","    \"\"\"\n","\n","    # Convert text to lowercase\n","    cleaned_text = text.lower()\n","\n","    # Remove content inside square brackets\n","    cleaned_text = re.sub(r'\\[.*?\\]', '', cleaned_text)\n","\n","    # Remove punctuation marks\n","    cleaned_text = re.sub(f\"[{re.escape(string.punctuation)}]\", '', cleaned_text)\n","\n","    # Remove words containing numbers\n","    cleaned_text = re.sub(r'\\b\\w*\\d\\w*\\b', '', cleaned_text)\n","\n","    return cleaned_text\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zgOu8t8HJrFz"},"source":["# Load the English NLP model\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","def extract_nouns(text):\n","    \"\"\"\n","    This function performs:\n","    - Lemmatization of text\n","    - Extracts POS tags and keeps only nouns (NN)\n","    \"\"\"\n","\n","    # Process text with spaCy\n","    doc = nlp(text)\n","\n","    # Extract lemmas only for nouns (NN)\n","    noun_lemmas = [token.lemma_ for token in doc if token.pos_ == \"NOUN\"]\n","\n","    # Return the processed text as a string\n","    return \" \".join(noun_lemmas)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uXnN7aa_JrF0","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1740352994025,"user_tz":-480,"elapsed":733097,"user":{"displayName":"Michael Thomas","userId":"16912601426565409661"}},"outputId":"1877641e-c447-478b-91fa-febd3ace7d92"},"source":["#Create a dataframe('df_clean') that will have only the complaints and the lemmatized complaints\n","\n","# Create a new DataFrame for cleaned complaints\n","df_clean = pd.DataFrame()\n","\n","# Apply the text preprocessing function\n","df_clean[\"complaint_what_happened\"] = df[\"complaint_what_happened\"].dropna().apply(preprocess_text)\n","\n","# Apply the lemmatization + noun extraction function\n","df_clean[\"lemmatized_complaint\"] = df_clean[\"complaint_what_happened\"].apply(extract_nouns)\n","\n","# Display the first few rows of the new DataFrame\n","df_clean.head()\n","\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                              complaint_what_happened  \\\n","1   good morning my name is xxxx xxxx and i apprec...   \n","2   i upgraded my xxxx xxxx card in  and was told ...   \n","10  chase card was reported on  however fraudulent...   \n","11  on  while trying to book a xxxx  xxxx  ticket ...   \n","14  my grand son give me check for  i deposit it i...   \n","\n","                                 lemmatized_complaint  \n","1   morning name stop bank cardmember service debt...  \n","2   xxxx card agent anniversary date agent informa...  \n","10  card application identity consent service cred...  \n","11  xxxx ticket offer ticket reward card informati...  \n","14  son check chase account fund chase bank accoun...  "],"text/html":["\n","  <div id=\"df-80c6a7d0-d51e-43c8-a0b2-7f3121f8b3fb\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>complaint_what_happened</th>\n","      <th>lemmatized_complaint</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>good morning my name is xxxx xxxx and i apprec...</td>\n","      <td>morning name stop bank cardmember service debt...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>i upgraded my xxxx xxxx card in  and was told ...</td>\n","      <td>xxxx card agent anniversary date agent informa...</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>chase card was reported on  however fraudulent...</td>\n","      <td>card application identity consent service cred...</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>on  while trying to book a xxxx  xxxx  ticket ...</td>\n","      <td>xxxx ticket offer ticket reward card informati...</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>my grand son give me check for  i deposit it i...</td>\n","      <td>son check chase account fund chase bank accoun...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-80c6a7d0-d51e-43c8-a0b2-7f3121f8b3fb')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-80c6a7d0-d51e-43c8-a0b2-7f3121f8b3fb button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-80c6a7d0-d51e-43c8-a0b2-7f3121f8b3fb');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-fc83e067-1e0d-4e1d-b3ef-03bf74c8a7c1\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fc83e067-1e0d-4e1d-b3ef-03bf74c8a7c1')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-fc83e067-1e0d-4e1d-b3ef-03bf74c8a7c1 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df_clean","summary":"{\n  \"name\": \"df_clean\",\n  \"rows\": 21072,\n  \"fields\": [\n    {\n      \"column\": \"complaint_what_happened\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20904,\n        \"samples\": [\n          \"on  last week i had confirmed with the bank i dont have no collection or balance that prevent of me to open an account \\n\\nhowever when i trying explain the bank doesnt accept it and still not able open account forever \\n\\nthe bank refused to assist me even after the letter i got from bank they arent cooperate me to insisted to open an online bank account only the problem is they wont do anything to open an account even if i knew that i dont own any balance from them\",\n          \"it says taht my xxxx card took a hard inquiry hit last xxxx i didnt do this why why would i need to i have plenty of money avail on taht card please remove herd inquiry wasnt me\",\n          \"my checking account at chase was compromised by someone printing and cashing fake checks that were not in my name but had my routing and account number at the bottom the checks were printed as if from various companies like insurance and legal companies checks did not have my name or signature anywhere see an example attached \\n\\nin total about  were lost from my account due to this \\n\\nin a span of three days from  through  hundreds of such fake checks were cleared from my checking account normally i would rarely write any checks in the past i wrote probably one check a month or so\\n\\nchase proceeded to approve these transactions and cleared funds from my account i was alerted to this happening by automated email from chase about my account being overdrawn in addition to fraudulent transactions chase charged three separate overdraft fees to my account \\n\\nupon receiving an alert and realizing the problem on  i contacted chase by phone then visited the branch where i opened a claim with chase chase froze the account and informed that it would take  days to process my claim\\n\\ntwo weeks later as of  chase has not refunded my money  around   and is claiming that the investigation is still ongoing \\n\\ni asked why chase can not proceed with reimbursing me while their investigation is ongoing but did not receive a satisfactory response \\n\\nessentially the same can happen to anybody with a chase checking account as its apparent that chase doesnt have any automated monitoring or alerting around suspicious checking transactions does not verify that checks even have an account holder s name or address on them and does not verify signatures\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lemmatized_complaint\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20753,\n        \"samples\": [\n          \"estate loan mortgage broker loan chase end buyer income chase kind background check borrower income credit profile borrower specific loan response couple reason chase explanation mortgage bank effect distrust broker company year merchant account bank business xxxx background check business merchant account branch manager week statement avail branch manager company xxxx issue bank past xxxx company explanation contact method response\",\n          \"debit bank card bank card paper work card\",\n          \"description home mortgage mortgage banker contract interest rate apr pmi cost month loan estimate number day closing date xxxxxxxx house appraisal xxxxxxxx value house result loan loan loan value client care pmi couple dollar result change month pmi communication mortgage banker problem loan condition issue attention day house disclosure pmi month mortgage banker supervisor kind error hand party mortgage insurance company pmi rate loan term document home day closing date client way change xxxxxxxx loan condition information information day xxxxxxxx certificate agency client care specialist figure guess pmi information fashion matter fact month reassurance case pmi rate resolution mortgage banker client care specialist supervisor phone call issue house option house deal document duress choice issue closing date amount advertising failure matter issue occasion supervisor xxxx pmi payment amount compensation amount amount overcharge year period issue\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"nOiDVvEIJrF0","colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"status":"ok","timestamp":1740354017775,"user_tz":-480,"elapsed":282,"user":{"displayName":"Michael Thomas","userId":"16912601426565409661"}},"outputId":"89dcaab0-f9d9-4105-8318-be2852189172"},"source":["df_clean"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                 complaint_what_happened  \\\n","1      good morning my name is xxxx xxxx and i apprec...   \n","2      i upgraded my xxxx xxxx card in  and was told ...   \n","10     chase card was reported on  however fraudulent...   \n","11     on  while trying to book a xxxx  xxxx  ticket ...   \n","14     my grand son give me check for  i deposit it i...   \n","...                                                  ...   \n","78303  after being a chase card customer for well ove...   \n","78309  on wednesday xxxxxxxx i called chas my xxxx xx...   \n","78310  i am not familiar with xxxx pay and did not un...   \n","78311  i have had flawless credit for  yrs ive had ch...   \n","78312  roughly  years ago i closed out my accounts wi...   \n","\n","                                    lemmatized_complaint  \n","1      morning name stop bank cardmember service debt...  \n","2      xxxx card agent anniversary date agent informa...  \n","10     card application identity consent service cred...  \n","11     xxxx ticket offer ticket reward card informati...  \n","14     son check chase account fund chase bank accoun...  \n","...                                                  ...  \n","78303  chase card customer decade solicitation credit...  \n","78309  xxxx credit card provider claim purchase prote...  \n","78310  xxxx pay risk consumer chase bank app chase ye...  \n","78311  credit yrs chase credit card chase freedom pro...  \n","78312  year account order line credit account payment...  \n","\n","[21072 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-1b04a1a9-42ba-429d-81d7-479b1386d671\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>complaint_what_happened</th>\n","      <th>lemmatized_complaint</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>good morning my name is xxxx xxxx and i apprec...</td>\n","      <td>morning name stop bank cardmember service debt...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>i upgraded my xxxx xxxx card in  and was told ...</td>\n","      <td>xxxx card agent anniversary date agent informa...</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>chase card was reported on  however fraudulent...</td>\n","      <td>card application identity consent service cred...</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>on  while trying to book a xxxx  xxxx  ticket ...</td>\n","      <td>xxxx ticket offer ticket reward card informati...</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>my grand son give me check for  i deposit it i...</td>\n","      <td>son check chase account fund chase bank accoun...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>78303</th>\n","      <td>after being a chase card customer for well ove...</td>\n","      <td>chase card customer decade solicitation credit...</td>\n","    </tr>\n","    <tr>\n","      <th>78309</th>\n","      <td>on wednesday xxxxxxxx i called chas my xxxx xx...</td>\n","      <td>xxxx credit card provider claim purchase prote...</td>\n","    </tr>\n","    <tr>\n","      <th>78310</th>\n","      <td>i am not familiar with xxxx pay and did not un...</td>\n","      <td>xxxx pay risk consumer chase bank app chase ye...</td>\n","    </tr>\n","    <tr>\n","      <th>78311</th>\n","      <td>i have had flawless credit for  yrs ive had ch...</td>\n","      <td>credit yrs chase credit card chase freedom pro...</td>\n","    </tr>\n","    <tr>\n","      <th>78312</th>\n","      <td>roughly  years ago i closed out my accounts wi...</td>\n","      <td>year account order line credit account payment...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>21072 rows × 2 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b04a1a9-42ba-429d-81d7-479b1386d671')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-1b04a1a9-42ba-429d-81d7-479b1386d671 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-1b04a1a9-42ba-429d-81d7-479b1386d671');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-398489c1-0dda-4398-af24-55d114ba8c8c\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-398489c1-0dda-4398-af24-55d114ba8c8c')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-398489c1-0dda-4398-af24-55d114ba8c8c button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_7620028b-a88e-4f91-acb3-cca3f957504c\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_clean')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_7620028b-a88e-4f91-acb3-cca3f957504c button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df_clean');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df_clean","summary":"{\n  \"name\": \"df_clean\",\n  \"rows\": 21072,\n  \"fields\": [\n    {\n      \"column\": \"complaint_what_happened\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20904,\n        \"samples\": [\n          \"on  last week i had confirmed with the bank i dont have no collection or balance that prevent of me to open an account \\n\\nhowever when i trying explain the bank doesnt accept it and still not able open account forever \\n\\nthe bank refused to assist me even after the letter i got from bank they arent cooperate me to insisted to open an online bank account only the problem is they wont do anything to open an account even if i knew that i dont own any balance from them\",\n          \"it says taht my xxxx card took a hard inquiry hit last xxxx i didnt do this why why would i need to i have plenty of money avail on taht card please remove herd inquiry wasnt me\",\n          \"my checking account at chase was compromised by someone printing and cashing fake checks that were not in my name but had my routing and account number at the bottom the checks were printed as if from various companies like insurance and legal companies checks did not have my name or signature anywhere see an example attached \\n\\nin total about  were lost from my account due to this \\n\\nin a span of three days from  through  hundreds of such fake checks were cleared from my checking account normally i would rarely write any checks in the past i wrote probably one check a month or so\\n\\nchase proceeded to approve these transactions and cleared funds from my account i was alerted to this happening by automated email from chase about my account being overdrawn in addition to fraudulent transactions chase charged three separate overdraft fees to my account \\n\\nupon receiving an alert and realizing the problem on  i contacted chase by phone then visited the branch where i opened a claim with chase chase froze the account and informed that it would take  days to process my claim\\n\\ntwo weeks later as of  chase has not refunded my money  around   and is claiming that the investigation is still ongoing \\n\\ni asked why chase can not proceed with reimbursing me while their investigation is ongoing but did not receive a satisfactory response \\n\\nessentially the same can happen to anybody with a chase checking account as its apparent that chase doesnt have any automated monitoring or alerting around suspicious checking transactions does not verify that checks even have an account holder s name or address on them and does not verify signatures\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lemmatized_complaint\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20753,\n        \"samples\": [\n          \"estate loan mortgage broker loan chase end buyer income chase kind background check borrower income credit profile borrower specific loan response couple reason chase explanation mortgage bank effect distrust broker company year merchant account bank business xxxx background check business merchant account branch manager week statement avail branch manager company xxxx issue bank past xxxx company explanation contact method response\",\n          \"debit bank card bank card paper work card\",\n          \"description home mortgage mortgage banker contract interest rate apr pmi cost month loan estimate number day closing date xxxxxxxx house appraisal xxxxxxxx value house result loan loan loan value client care pmi couple dollar result change month pmi communication mortgage banker problem loan condition issue attention day house disclosure pmi month mortgage banker supervisor kind error hand party mortgage insurance company pmi rate loan term document home day closing date client way change xxxxxxxx loan condition information information day xxxxxxxx certificate agency client care specialist figure guess pmi information fashion matter fact month reassurance case pmi rate resolution mortgage banker client care specialist supervisor phone call issue house option house deal document duress choice issue closing date amount advertising failure matter issue occasion supervisor xxxx pmi payment amount compensation amount amount overcharge year period issue\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"Kk7fc4DuJrF1","colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"status":"error","timestamp":1740435493176,"user_tz":-480,"elapsed":7786,"user":{"displayName":"Michael Thomas","userId":"16912601426565409661"}},"outputId":"39e48c00-e3e6-42c3-dc91-0d4fc839fc90"},"source":["#Write your function to extract the POS\n","\n","\n","import spacy\n","import pandas as pd\n","\n","# Load the spaCy model\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","def extract_singular_nouns(text):\n","    \"\"\"\n","    Extracts only singular nouns (NN) from the given text using spaCy.\n","    \"\"\"\n","\n","    # Process the text with spaCy\n","    doc = nlp(text)\n","\n","    # Keep only words tagged as singular nouns (NN)\n","    noun_only_text = \" \".join([token.text for token in doc if token.tag_ == \"NN\"])\n","\n","    return noun_only_text\n","\n","# Apply the function to filter only singular nouns and store in a new column\n","df_clean[\"complaint_POS_removed\"] = df_clean[\"lemmatized_complaint\"].apply(extract_singular_nouns)\n","\n","# Display the first few rows of the updated DataFrame\n","df_clean.head()\n","\n"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'df_clean' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-79b954d41d4f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Apply the function to filter only singular nouns and store in a new column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mdf_clean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"complaint_POS_removed\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_clean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lemmatized_complaint\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_singular_nouns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Display the first few rows of the updated DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df_clean' is not defined"]}]},{"cell_type":"code","metadata":{"id":"AjxfchvFJrF2","executionInfo":{"status":"error","timestamp":1740435534111,"user_tz":-480,"elapsed":37,"user":{"displayName":"Michael Thomas","userId":"16912601426565409661"}},"colab":{"base_uri":"https://localhost:8080/","height":159},"outputId":"46342601-8b2d-4028-e5e2-25e30b5a2dc3"},"source":["#The clean dataframe should now contain the raw complaint, lemmatized complaint and the complaint after removing POS tags.\n","df_clean"],"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'df_clean' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-aacafc6cbb3a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#The clean dataframe should now contain the raw complaint, lemmatized complaint and the complaint after removing POS tags.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_clean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'df_clean' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"_7Un1AElJrF2"},"source":["## Exploratory data analysis to get familiar with the data.\n","\n","Write the code in this task to perform the following:\n","\n","*   Visualise the data according to the 'Complaint' character length\n","*   Using a word cloud find the top 40 words by frequency among all the articles after processing the text\n","*   Find the top unigrams,bigrams and trigrams by frequency among all the complaints after processing the text. ‘\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"q-zaqJF6JrF2"},"source":["# Write your code here to visualise the data according to the 'Complaint' character length\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Calculate character length of each complaint\n","df_clean[\"complaint_length\"] = df_clean[\"complaint_POS_removed\"].str.len()\n","\n","# Plot distribution\n","plt.figure(figsize=(12,6))\n","sns.histplot(df_clean[\"complaint_length\"], bins=30, kde=True, color=\"purple\")\n","plt.title(\"Distribution of Complaint Lengths\", fontsize=14)\n","plt.xlabel(\"Number of Characters\", fontsize=12)\n","plt.ylabel(\"Frequency\", fontsize=12)\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T9jD_6SeJrF3"},"source":["#### Find the top 40 words by frequency among all the articles after processing the text."]},{"cell_type":"code","metadata":{"id":"QcfdvtfZJrF3"},"source":["#Using a word cloud find the top 40 words by frequency among all the articles after processing the text\n","import matplotlib.pyplot as plt\n","from wordcloud import WordCloud, STOPWORDS\n","\n","# Define a set of words to exclude\n","stopwords = set(STOPWORDS)\n","\n","# Create a WordCloud with a limit of 40 words\n","wordcloud = WordCloud(\n","    background_color=\"white\",\n","    stopwords=stopwords,\n","    max_words=40,\n","    max_font_size=40,\n","    random_state=42\n",").generate(\" \".join(df_clean[\"complaint_POS_removed\"]))\n","\n","# Plot and display the word cloud\n","plt.figure(figsize=(12, 8))\n","plt.imshow(wordcloud, interpolation=\"bilinear\")\n","plt.axis(\"off\")  # Hide axis for cleaner look\n","plt.title(\"Word Cloud of Top 40 Words\", fontsize=16)\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OkSmc3UaJrF4"},"source":["#Removing -PRON- from the text corpus\n","df_clean['Complaint_clean'] = df_clean['complaint_POS_removed'].str.replace('-PRON-', '')"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_clean.head()"],"metadata":{"id":"dVTBwkLCE9qZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5DfCSbbmJrF4"},"source":["#### Find the top unigrams,bigrams and trigrams by frequency among all the complaints after processing the text."]},{"cell_type":"code","metadata":{"id":"5mbk5DS5JrF4"},"source":["#Write your code here to find the top 30 unigram frequency among the complaints in the cleaned datafram(df_clean).\n","\n","\n","def get_top_n_grams(text_data, n=1, top_k=10):\n","    \"\"\"\n","    Extracts the top-k most frequent n-grams (unigrams, bigrams, or trigrams) from the given text.\n","\n","    Parameters:\n","    -----------\n","    text_data : list or Pandas Series\n","        The corpus of texts from which to extract n-grams.\n","    n : int\n","        Size of the n-gram (1 = unigrams, 2 = bigrams, 3 = trigrams, etc.).\n","    top_k : int\n","        Number of top n-grams to retrieve.\n","\n","    Returns:\n","    --------\n","    list of tuples (ngram, frequency)\n","        Each tuple contains the n-gram and its overall frequency in the corpus.\n","    \"\"\"\n","\n","    vectorizer = CountVectorizer(ngram_range=(n, n), stop_words='english')\n","    transformed_data = vectorizer.fit_transform(text_data)\n","\n","    # Sum the counts of each n-gram across all documents\n","    word_counts = transformed_data.sum(axis=0)\n","\n","    # Map each n-gram to its frequency\n","    freq_map = [\n","        (ngram, word_counts[0, idx])\n","        for ngram, idx in vectorizer.vocabulary_.items()\n","    ]\n","\n","    # Sort by frequency in descending order\n","    sorted_freq_map = sorted(freq_map, key=lambda x: x[1], reverse=True)\n","\n","    # Return the top_k n-grams\n","    return sorted_freq_map[:top_k]\n","\n","# Example usage: extracting unigrams, bigrams, and trigrams\n","top_20_unigrams = get_top_n_grams(df_clean[\"complaint_POS_removed\"], n=1, top_k=20)\n","top_20_bigrams = get_top_n_grams(df_clean[\"complaint_POS_removed\"], n=2, top_k=20)\n","top_20_trigrams = get_top_n_grams(df_clean[\"complaint_POS_removed\"], n=3, top_k=20)\n","\n","# Converting results to DataFrames for easy viewing\n","df_unigrams = pd.DataFrame(top_20_unigrams, columns=[\"Unigram\", \"Frequency\"])\n","df_bigrams = pd.DataFrame(top_20_bigrams, columns=[\"Bigram\", \"Frequency\"])\n","df_trigrams = pd.DataFrame(top_20_trigrams, columns=[\"Trigram\", \"Frequency\"])\n","\n","print(\"Top 20 Unigrams:\\n\", df_unigrams)\n","print(\"\\nTop 20 Bigrams:\\n\", df_bigrams)\n","print(\"\\nTop 20 Trigrams:\\n\", df_trigrams)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YX7fedm1JrF8"},"source":["#Print the top 10 words in the unigram frequency\n","\n","# Display the top 10 unigrams\n","print(\"Top 10 Unigrams:\")\n","print(df_unigrams.head(10))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aV7kD7w8JrF8"},"source":["#Write your code here to find the top 30 bigram frequency among the complaints in the cleaned datafram(df_clean).\n","import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","def get_top_bigrams(text_data, top_k=30):\n","    \"\"\"\n","    Returns the most common bigrams (two-word sequences) in the given text corpus.\n","\n","    Parameters:\n","    -----------\n","    text_data : list or pd.Series\n","        Collection of documents (strings) to analyze.\n","    top_k : int\n","        The number of top bigrams to retrieve.\n","\n","    Returns:\n","    --------\n","    list of tuples [(bigram, frequency), ...]\n","        Sorted in descending order by frequency.\n","    \"\"\"\n","\n","    # Initialize CountVectorizer for bigrams\n","    vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words=\"english\")\n","    transformed_data = vectorizer.fit_transform(text_data)\n","\n","    # Sum the counts of each bigram\n","    counts = transformed_data.sum(axis=0)\n","\n","    # Create a list of (bigram, frequency) pairs\n","    bigram_freqs = [\n","        (bigram, counts[0, idx])\n","        for bigram, idx in vectorizer.vocabulary_.items()\n","    ]\n","\n","    # Sort in descending order by frequency\n","    sorted_bigram_freqs = sorted(bigram_freqs, key=lambda x: x[1], reverse=True)\n","\n","    return sorted_bigram_freqs[:top_k]\n","\n","# Use the function to get the top 30 bigrams in the processed text\n","bigrams_top_30 = get_top_bigrams(df_clean[\"complaint_POS_removed\"].values.astype(\"U\"), top_k=30)\n","\n","# Convert the result into a DataFrame\n","df_bigrams = pd.DataFrame(bigrams_top_30, columns=[\"Bigram\", \"Frequency\"])\n","\n","# Display the top 30 bigrams\n","print(df_bigrams)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NPnMNIpyJrF9"},"source":["#Print the top 10 words in the bigram frequency\n","print(\"\\nTop 10 Bigram Words:\")\n","print(df_bigrams[\"Bigram\"].head(10))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xkh7vtbtJrF-"},"source":["#Write your code here to find the top 30 trigram frequency among the complaints in the cleaned datafram(df_clean).\n","import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","def get_top_trigrams(corpus, top_k=30):\n","    \"\"\"\n","    Extracts the top_k most frequent trigrams from a given text corpus.\n","\n","    Parameters:\n","      corpus (list or pd.Series): The text data to analyze.\n","      top_k (int): The number of top trigrams to return.\n","\n","    Returns:\n","      list of tuples: Each tuple contains a trigram and its frequency.\n","    \"\"\"\n","    # Initialize CountVectorizer for trigrams (3-word sequences)\n","    vectorizer = CountVectorizer(ngram_range=(3, 3), stop_words='english')\n","    X = vectorizer.fit_transform(corpus)\n","\n","    # Sum up the counts for each trigram\n","    trigram_counts = X.sum(axis=0)\n","\n","    # Map trigrams to their frequencies\n","    trigram_freqs = [\n","        (trigram, trigram_counts[0, idx])\n","        for trigram, idx in vectorizer.vocabulary_.items()\n","    ]\n","\n","    # Sort the list by frequency in descending order\n","    trigram_freqs_sorted = sorted(trigram_freqs, key=lambda x: x[1], reverse=True)\n","\n","    return trigram_freqs_sorted[:top_k]\n","\n","# Apply the function to the processed text column in df_clean\n","top_30_trigrams = get_top_trigrams(df_clean['complaint_POS_removed'].values.astype('U'), top_k=30)\n","\n","# Convert the results to a DataFrame for easy viewing\n","df_trigrams = pd.DataFrame(top_30_trigrams, columns=['trigram', 'count'])\n","\n","# Print the top 30 trigrams\n","print(df_trigrams)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"REcVxNfvJrF-"},"source":["#Print the top 10 words in the trigram frequency\n","\n","print(\"Top 10 Trigrams with Frequency:\")\n","print(df_trigrams.head(10))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yUXzFji0JrF_"},"source":["## The personal details of customer has been masked in the dataset with xxxx. Let's remove the masked text as this will be of no use for our analysis"]},{"cell_type":"code","metadata":{"id":"wKda-a_IJrF_"},"source":["df_clean['Complaint_clean'] = df_clean['Complaint_clean'].str.replace('xxxx','')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9UIFk8fQJrF_"},"source":["#All masked texts has been removed\n","df_clean"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k-I0k0QtJrGA"},"source":["## Feature Extraction\n","Convert the raw texts to a matrix of TF-IDF features\n","\n","**max_df** is used for removing terms that appear too frequently, also known as \"corpus-specific stop words\"\n","max_df = 0.95 means \"ignore terms that appear in more than 95% of the complaints\"\n","\n","**min_df** is used for removing terms that appear too infrequently\n","min_df = 2 means \"ignore terms that appear in less than 2 complaints\""]},{"cell_type":"code","metadata":{"id":"Y8fGwaCPJrGA"},"source":["#Write your code here to initialise the TfidfVectorizer\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yYzD85nTJrGA"},"source":["#### Create a document term matrix using fit_transform\n","\n","The contents of a document term matrix are tuples of (complaint_id,token_id) tf-idf score:\n","The tuples that are not there have a tf-idf score of 0"]},{"cell_type":"code","metadata":{"id":"ffzdDpp_JrGB"},"source":["#Write your code here to create the Document Term Matrix by transforming the complaints column present in df_clean.\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Q9lwvNEJrGB"},"source":["## Topic Modelling using NMF\n","\n","Non-Negative Matrix Factorization (NMF) is an unsupervised technique so there are no labeling of topics that the model will be trained on. The way it works is that, NMF decomposes (or factorizes) high-dimensional vectors into a lower-dimensional representation. These lower-dimensional vectors are non-negative which also means their coefficients are non-negative.\n","\n","In this task you have to perform the following:\n","\n","* Find the best number of clusters\n","* Apply the best number to create word clusters\n","* Inspect & validate the correction of each cluster wrt the complaints\n","* Correct the labels if needed\n","* Map the clusters to topics/cluster names"]},{"cell_type":"code","metadata":{"id":"amLT4omWJrGB"},"source":["from sklearn.decomposition import NMF"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0wYR1xUTJrGD"},"source":["## Manual Topic Modeling\n","You need to do take the trial & error approach to find the best num of topics for your NMF model.\n","\n","The only parameter that is required is the number of components i.e. the number of topics we want. This is the most crucial step in the whole topic modeling process and will greatly affect how good your final topics are."]},{"cell_type":"code","metadata":{"id":"sgd2A6bhJrGD"},"source":["#Load your nmf_model with the n_components i.e 5\n","num_topics = #write the value you want to test out\n","\n","#keep the random_state =40\n","nmf_model = #write your code here"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VPMDYbt_JrGE"},"source":["nmf_model.fit(dtm)\n","len(tfidf.get_feature_names())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"16kRfat5JrGE"},"source":["#Print the Top15 words for each of the topics\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0OIT7LmFJrGF"},"source":["#Create the best topic for each complaint in terms of integer value 0,1,2,3 & 4\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"peyYv-ORJrGF"},"source":["#Assign the best topic to each of the cmplaints in Topic Column\n","\n","df_clean['Topic'] = #write your code to assign topics to each rows."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fLh_Gf3nJrGF"},"source":["df_clean.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aQKpufSPJrGG"},"source":["#Print the first 5 Complaint for each of the Topics\n","df_clean=df_clean.groupby('Topic').head(5)\n","df_clean.sort_values('Topic')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"piyLxzj6v07j"},"source":["#### After evaluating the mapping, if the topics assigned are correct then assign these names to the relevant topic:\n","* Bank Account services\n","* Credit card or prepaid card\n","* Theft/Dispute Reporting\n","* Mortgage/Loan\n","* Others"]},{"cell_type":"code","metadata":{"id":"TWpwDG4RJrGG"},"source":["#Create the dictionary of Topic names and Topics\n","\n","Topic_names = {   }\n","#Replace Topics with Topic Names\n","df_clean['Topic'] = df_clean['Topic'].map(Topic_names)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-2ULY5K6JrGG"},"source":["df_clean"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Mu0QBOcJrGH"},"source":["## Supervised model to predict any new complaints to the relevant Topics.\n","\n","You have now build the model to create the topics for each complaints.Now in the below section you will use them to classify any new complaints.\n","\n","Since you will be using supervised learning technique we have to convert the topic names to numbers(numpy arrays only understand numbers)"]},{"cell_type":"code","metadata":{"id":"_U8J3J8wJrGH"},"source":["#Create the dictionary again of Topic names and Topics\n","\n","Topic_names = {   }\n","#Replace Topics with Topic Names\n","df_clean['Topic'] = df_clean['Topic'].map(Topic_names)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BWIgJUkQJrGH"},"source":["df_clean"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xx-FrbkWJrGH"},"source":["#Keep the columns\"complaint_what_happened\" & \"Topic\" only in the new dataframe --> training_data\n","training_data="],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lVg2pa12JrGI"},"source":["training_data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"280Vbqk-7a8M"},"source":["####Apply the supervised models on the training data created. In this process, you have to do the following:\n","* Create the vector counts using Count Vectoriser\n","* Transform the word vecotr to tf-idf\n","* Create the train & test data using the train_test_split on the tf-idf & topics\n"]},{"cell_type":"code","metadata":{"id":"oUlQpgkzJrGI"},"source":["\n","#Write your code to get the Vector count\n","\n","\n","#Write your code here to transform the word vector to tf-idf"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uMU3vj6w-wqL"},"source":["You have to try atleast 3 models on the train & test data from these options:\n","* Logistic regression\n","* Decision Tree\n","* Random Forest\n","* Naive Bayes (optional)\n","\n","**Using the required evaluation metrics judge the tried models and select the ones performing the best**"]},{"cell_type":"code","metadata":{"id":"udLHpPsZJrGI"},"source":["# Write your code here to build any 3 models and evaluate them using the required metrics\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N2OznsObJrGP"},"source":[],"execution_count":null,"outputs":[]}]}